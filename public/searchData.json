[{"title":"Using Kafka as a Event Store","url":"/2019/02/10/Using-Kafka-as-a-Event-Store/","content":"Kafka is meant to be a messaging system which has many similarities to an event store however to quote their intro:\n\n> The Kafka cluster retains all published messages—whether or not they\n> have been consumed—**for a configurable period of time**. For example if\n> the retention is set for two days, then for the two days after a\n> message is published it is available for consumption, after which it\n> will be discarded to free up space. Kafka's performance is effectively\n> constant with respect to data size so retaining lots of data is not a\n> problem.\n\nSo while messages can potentially be retained indefinitely, the expectation is that they will be deleted. This doesn't mean you can't use this as an event store, but it may be better to use something else. Take a look at [EventStore][1] for an alternative.\n\n[Kafka documentation](http://kafka.apache.org/documentation.html):\n\n> Event sourcing is a style of application design where state changes are logged as a time-ordered sequence of records. Kafka's support for very large stored log data makes it an excellent backend for an application built in this style.\n\nOne concern with using Kafka for event sourcing is the number of required topics. Typically in event sourcing, there is a stream (topic) of events per entity (such as user, product, etc). This way, the current state of an entity can be reconstituted by re-applying all events in the stream. Each Kafka topic consists of one or more partitions and each partition is stored as a directory on the file system. There will also be pressure from ZooKeeper as the number of znodes increases.\n\nOther things that we should notice are:\n\n - Kafka only guarantees at least once deliver and there are duplicates\n   in the event store that cannot be removed. \n   Here you can read why it is so hard with Kafka and some latest news about how to finally achieve this behavior: https://www.confluent.io/blog/exactly-once-semantics-are-possible-heres-how-apache-kafka-does-it/\n - Due to immutability, there is no way to manipulate event store when application evolves and events need to be transformed (there are of course methods like upcasting, but...). Once might say you never need to transform events, but that is not correct assumption, there could be situation where you do backup of original, but you upgrade them to latest versions. That is valid requirement in event driven architectures.\n - No place to persist snapshots of entities/aggregates and replay will become slower and slower. Creating snapshots is must feature for event store from long term perspective. \n - Given Kafka partitions are distributed and they are hard to manage and\n   backup compare with databases. Databases are simply simpler\n\nSo, before you make your choice you think twice. Event store as combination of application layer interfaces (monitoring and management), SQL/NoSQL store and Kafka as broker is better choice than leaving Kafka handle both roles to create complete feature full solution.\n\nEvent store is complex service which requires more than what Kafka can offer if you are serious about applying Event sourcing, CQRS, Sagas and other patterns in event driven architecture and stay high performance.\n\nPlease look at eventuate.io microservices open source framework to discover more about the potential problems: http://eventuate.io/\n\n### Back to Topic\n- Using Kafka as a event soursing? Yes or No, depending on your event sourcing usage.\n- It can be used in downstream event processors\nIn this kind of system, events happen in the real world and are recorded as facts. Such as a warehouse system to keep track of pallets of products. There are basically no conflicting events. Everything has already happened, even if it was wrong. (I.e. pallet 123456 put on truck A, but was scheduled for truck B.) Then later the facts are checked for exceptions via reporting mechanisms. Kafka seems well-suited for this kind of down-stream, event processing application.\n\nIn this context, it is understandable why Kafka folks are advocating it as an Event Sourcing solution. Because it is quite similar to how it is already used in, for example, click streams. However, people using the term Event Sourcing (as opposed to Stream Processing) are likely referring to the second usage...\n- It CANNOT be used in Application-controlled source of truth\n\nThis kind of application declares its own events as a result of user requests passing through business logic. Kafka does not work well in this case for two primary reasons.\n+ Lack of entity isolation\n\nThis scenario needs the ability to load the event stream for a specific entity. The common reason for this is to build a transient write model for the business logic to use to process the request. Doing this is impractical in Kafka. Using topic-per-entity could allow this, except this is a non-starter when there may be thousands or millions of that entity. This is due to technical limits in Kafka/Zookeeper. Using topic-per-type is recommended instead for Kafka, but this would require loading events for every entity of that type just to get events for a single entity. Since you cannot tell by log position which events belong to which entity. Even using Snapshots to start from a known log position, this could be a significant number of events to churn through. But snapshots cannot help you with code changes. Because adding new features to the business logic may render previous snapshots structurally incompatible. So it is still necessary to do a topic replay in those cases to build a new model. One of the main reasons to use a transient write model instead of a persisted one is to make business logic changes cheap and easy to deploy.\n\n+ Lack of conflict detection\n\nSecondly, users can create race conditions due to concurrent requests against the same entity. It may be quite undesirable to save conflicting events and resolve them after the fact. So it is important to be able to prevent conflicting events. To scale request load, it is common to use stateless services while preventing write conflicts using conditional writes (only write if the last entity event was #x). A.k.a. Optimistic Concurrency. Kafka does not support optimistic concurrency. Even if it supported it at the topic level, it would need to be all the way down to the entity level to be effective. To use Kafka and prevent conflicting events, you would need to use a stateful, serialized writer at the application level. This is a significant architectural requirement/restriction.\n\n  [1]: http://geteventstore.com/","tags":["distributed System"],"categories":["Tech"]},{"title":"2019 Aspiration","url":"/2019/01/01/2019-Aspiration/","content":"\n新年的愿景\n\n应该算是结束20年来幼稚懵懂的那段时期。在我看来，至少是去年开始不再无忧无虑，想干什么干什么了。还好不算太晚。开始GTD的时候真的有点手足无措，但是慢慢觉得计划着做事情才算是增加有意义的熵。\n\n年底打开6年前的OneNote（是的6年前，那时候高中第一部手机WP上自带的），看到了之前抄下的一段话:\n\n```\n青春是一个短暂的美梦，当你醒来时，它早已消失无踪。by 莎士比亚\n```\n现在大概算是如梦初醒的时候。\n\n写下一些今年需要做的事情吧。然后记录在Things里面。\n\n+ \b拿到Offer\n+ 读完on writing well 和 writing matters\n+ 再读一遍the universe in nutshell\n+ 写/参与一个开源项目\n+ Jazz theory读完\n+ 完善计划\n\n就这么多了，太多了会让自己很难堪的。","tags":["flag"]},{"title":"Let your VPS be useful","url":"/2018/03/11/Let-your-VPS-be-useful/","content":"\nI'd tried a wide range of cloud vendor. Just because I feel little bit neat freak on my own machine. On the virtual machine, I don't worry about the overload, the wastage or the upgrade. That is the advantage of the VPS. When you want to add disk size due to the massive data, just few clicks can help you; or when you want to maintain some monitor some metrix on your machine, just keep it running and no need to focus the runtime of the physics device -- a wholesome cloud infrastructure will help you.\n\nHowever, most people just leave the machine unused, since the cloud vendor, after all, provided the most direct service for customer rather than left ourself to develop. But those service, more or less limiting the usage of single user, annoyed most of us. Why don't we just using the docker to build a own?\n\n## ownCloud\nFeel bad about the charging policy or limited size of the Google Drive or BaiduYunPan? It is undisputed for most public cloud drive markets that only subscribed can enjoy the tiny convenience of the cloud drive. \n\nHow about using ownCloud to build our own?\n\n    docker run -p 8081:80 -d imdjh/owncloud-with-ocdownloader\n\nJust one line code with docker can quickly build a private cloud drive.\n\n## MetaBase\nSince we got our cloud drive to storage our data, why don't we build a data analysis tool to handle our data on cloud?\n\n> Metabase is the easy, open source way for everyone in your company to ask questions and learn from data.\n\n    docker run -d -p 3000:3000 --name metabase metabase/metabase\n\n## minidlna\nWatching TV or PS4 all the day, or we can using the server turn into a mutil media center.\n\n    sudo apt-get install minidlna\n\nOnly if you TV supports dlna protocol\n\n## Cloud backup\nTime Capsule is good, build our own cloud backup using AFP protocol is also functional achievable. \n\n    sudo apt-get install netatalk avahi-daemon\n    sudo vim /etc/netatalk/AppleVolumes.default\n\nEdit some config file:\n\n    :DEFAULT: options:upriv,usedots\n    /home/exampleuser/tm \"TimeMachine\" options:tm exampleuser\n\n\n","tags":["Linux"],"categories":["Tech"]},{"title":"Using Tone.js to make 8 bit music","url":"/2018/03/01/Using-Tone-js-to-make-8bit-music/","content":"\n## What is 8 bit music\n8 bit music is also called chip music.\n\nAt that time, only small memory can be installed in game console （e.g. FC). Producer cannot make high sampling-rate PCM music. But music is necessary when we play games. One solution is making music real time, which the played music can quickly clean the used memory and rebuild the next note. \n\nWhen making 8 bit music, we should storage the basic sound into the device priority. In the game music, we also needs to upload our music code (like the score) into the program. In this way we can make a game music, which become more popular in nowdays and gradually forming a new style on electricity music genre.\n\n## How to make 8 bit mudic\n\nWe use FC (Family Computer) as an example to explain 8 bit music. In the music system of FC, instrument is not exist. It only provided different waves to producer. When composing, people needs to transform different waves into different audio effect.\n\n5 tracks in FC:\n\n1. square wave (2 track)\n\n![square wave](https://i.loli.net/2019/02/12/5c62ce1f5dc96.jpg)\n\nSquare wave occupy two track, since it has variable proportions waves. Thus the wave generate different tone. Piano, guitar and other instrument can be simulated by this wave.  \n\n2. triangular wave (1 track)\n\n![triangular wave](https://i.loli.net/2019/02/12/5c62ce1f6c3b1.jpg)\n\nsometime we use this wave to simulate bass\n\n3. noise (1 track)\n\nNoise is the most common audio effect in games. We use noise to create the environment sound, like explode, footstep, crash and the rhythmic tapping.\n\n4. Sampling (1 track)\n\nSampling track is more complex than other. We don't need to dig it all up. \n\n#### Note: How to modify a note\n\nUsually, we use volumn, trill or envelope (ADSR) to polish the sound. \n\n## Web Audio\n\nHere is a [website](https://codepen.io/anon/embed/LxJEaj?slug-hash=LxJEaj&default-tab=result&height=300&theme-id=0&embed-version=2&user=anon) to exhibit web audio: \n\nHowever, you may notice that FC didn't provide sin wave. Because sin/cos wave is published after the game console. It is more soft than normal FC sound. \n\nTone.js is open source web audio SDK for developer make music using JS. We choose Tong.js because:\n\n+ Tone.js accepted we edit pitch rather than ask us play 440Hz sound. For example, it packaged the scale into the library, we don't need to play a sound which exactly is 440Hz.\n+ It packaged Attack and Release in ADSR (Attack, Decay, Sustain, Release). More convenience to develop \n\n## Steps\n\n1. Essential functon in Tone.js for sounding.\n\n    .triggerAttackRelease()\n\nIt including four parameter: note, duration, time and velocity.\n\nTime is related to duration which represent the position of the note in a song; velocity is a detail change, sometime 8 bit music left this parameter unchanged.\n\nThe code below is a example to show how we use these four parameter to make music.\n\n    var synth = new Tone.Synth().toMaster()\n    synth.triggerAttackRelease('C4', '4n', '8n', 1)\n    synth.triggerAttackRelease('E4', '8n', '4n + 8n', 1)\n\nIn this snippets, C4 and E4 represent the frequent of the sound. Time is accumulate and velocity is fixed. \n\nHow about we use code to write one bar?\n\n    var synth = new Tone.Synth().toMaster()\n    Tone.Transport.bpm.value = 120\n\n    synth.triggerAttackRelease('E4', '4n', '0', 1)\n    synth.triggerAttackRelease('E4', '4n', '4n', 1)\n    synth.triggerAttackRelease('F4', '4n', '2n', 1)\n    synth.triggerAttackRelease('G4', '4n', '2n+4n', 1)\n\n2. Using harmony\n\nWhat is the harmony? It is two or more sound play together, following some rules. We can use different note at the same time to composite a new harmony. \n\nAfter finished the single track editing, we need to add more instruments.\n\n    var triangleOptions = {\n    oscillator: {\n        type: 'triangle'\n    }\n    }\n\n    var squareOptions = {\n    oscillator: {\n        type: 'square'\n    }\n    }\n\n    var squareSynth = new Tone.Synth(squareOptions).toMaster()\n\n    var triangleSynth = new Tone.Synth(triangleOptions).toMaster()\n\n    var noiseSynth = new Tone.NoiseSynth().toMaster()\n\nEach tone has different wave, which represent different instruments. `squareSynth` is used to play main melody instruments, such as piano, guitar. `trangleSynth` is used to simulate bass. `noiseSynth` is used to smulate percussion music。\n\n3. Audio effect\n\nLast step we should add audio effect to make music more rich. Like I mentioned before, ADSR can create some reverberation, echo and differnet pitch. These can be achieved by `Tone.Envelope()`.\n\n![ADSR](https://i.loli.net/2019/02/12/5c62ce1f5dc96.jpg)\n\nHere is the code:\n\n    envelope: {\n    attack  : 0.01 ,\n    decay  : 0.1 ,\n    sustain  : 0.5 ,\n    release  : 1 ,\n    attackCurve  : linear ,\n    releaseCurve  : exponential\n    }\n\n## Consolution\nPretty easy aha, I think the next generation of music must be computer music. ","tags":["music"],"categories":["Tech"]},{"title":"Different between 5 Hyperleder project","url":"/2018/02/10/hyperledger-project/","content":"The Linux Foundation’s Hyperledger project, which is focused on open source blockchain technology, divides its work into five sub projects. Hyperledger Executive Director Brian Behlendorf said Hyperledger’s technical steering committee must approve each new sub project, and it’s looking for projects that “represent different thinking.”\n\nThe first five projects are: Fabric, Sawtooth, Indy, Burrow, and Iroha.\n\n“Every one of these projects started life outside of Hyperledger, first, by a team that had certain use cases in mind,” said Behlendorf. Each project must bring something unique to the open source group, and its technology must be applicable to other companies.\n\n### Fabric\nFabric is Hyperledger’s most active project to date. The Fabric 1.0 release was issued in July. IBM initiated the Fabric project. It’s intended as a foundation for developing blockchain distributed ledger applications with a modular architecture. It allows components, such as consensus and membership services, to be plug-and-play.\n\n“Fabric is the granddaddy, if you will,” said Behlendorf. “Several companies are already selling products and services based on it.” The core of the platform is written in the Go programming language. A unique characteristic of Fabric is that its distributed ledger and smart contract platform allows for private channels. “If you have a large blockchain network and you want to share data with only certain parties, you can create a private channel with just those participants,” Behlendorf said. “It’s the most distinctive thing about Fabric right now.”\n\n### Sawtooth\nThe Sawtooth project originally came from Intel. It includes a novel consensus algorithm called Proof of Elapsed Time. Consensus is a critical element of all blockchains. Generally, it is the technique by which new information is reviewed and confirmed before being accepted as the next entry in the ledger.\n\nThe Sawtooth consensus software targets large distributed validator populations with minimal resource consumption. “It may give us the ability to build very broad and flat networks of hundreds to thousands of nodes,” said Behlendorf. “It’s harder to do with traditional consensus mechanisms without having the CPU burden of cryptocurrencies.”\n\n### Indy\nThe Indy project was originally the brainchild of the nonprofit group the Sovrin Foundation. The idea is to provide digital identities for individuals and give them the power to share their identity with whom they chose. “Instead of being an entry in a giant data base, you have your data and deal programmatically with different organizations who want to check your identity,” said Behlendorf. “And companies don’t have to store so much personal data. They can store a pointer to the identity.”\n\nIndy’s work looks especially timely, given the recent Experian hack. Behlendorf said Indy’s blockchain software is based on data minimization. When a company is done with your data, it throws it away. “It’s a toxic asset that could present a liability,” he said.\n\n### Burrow\nThe Burrow project includes a permissioned, smart-contract interpreter built in part to the specification of the Ethereum Virtual Machine (EVM). The Ethereum platform is used both for cryptocurrency as well as for smart contracts. It’s written with the Solidity programming language. Within the Burrow Project, the EVM is the interpreter for smart contracts (not related to cryptocurrency) that run across the Ethereum network.\n\nMany well-known companies belong to the Enterprise Ethereum Alliance, including JPMorgan, Microsoft, Accenture, BP, and Cisco.\n\n“It’s important to build a relationship with the Ethereum community,” said Behlendorf. “Burrow is the only Apache-licensed Ethereum VM implementation out there.”\n\n### Iroha\nFinally, the Iroha project is a bit of an outlier within Hyperledger. It originated with some developers in Japan who had built their own blockchain technology for a couple of mobile use cases. “It’s implemented in C++ which can be more high performance for small data and focused use cases,” said Behlendorf. “Iroha is still looking for its niche, but it’s a great development team.”","tags":["Hyperledger"],"categories":["Tech"]},{"title":"Programming language for Distribute System","url":"/2018/02/09/programming-language-for-distributed-system/","content":"## 1. Erlang\n[Erlang][1], as described by [Wikipedia][2]:\n\n> It was designed by Ericsson to support distributed, fault-tolerant, soft-real-time, non-stop applications.\n\nYou might also want to read the [*Distributed Erlang*][3] section of their manual.\n\nHowever, note that Erlang is a [*functional* language][4] and will require a much different paradigm of thought as compared to C++.\n\n> A distributed Erlang system consists of a number of Erlang runtime systems communicating with each other. Each such runtime system is called a node. Message passing between processes at different nodes, as well as links and monitors, are transparent when pids are used. Registered names, however, are local to each node. This means that the node must be specified as well when sending messages, and so on, using registered names.\n\n\n\n\n\n\n## 2. Golang\nGoLang from Google is a pretty new language. It seems that among its many attributes, it may some day be suitable for large distributed systems requiring a lot of message queues to achieve scalable consistent and reliable behaviours, at least according to [these folks][5] at heroku.\n\nGo seems to be focused on concurrency issues, threading primitives in the language, and so on, and this is perhaps a necessary-but-not-quite-sufficient starting point for distributed systems. Perhaps their thoughts will be helpful to you. I wouldn't call Go-lang's support for distributed systems \"first-class\", but rather, say that it would be possible to build a first class distributed-systems framework using Go's library and language primitives.\n\nAt first, I'm less impressed with Go. I think it suffers from some sad and limited thinking on the part of its authors. I think its decisions on fault and exception handling are retrograde, and render the language unusable.\n\nBut I now think in terms of large team development where having N-factorial implementation options leads to N-factorial different coding tarpits. At least Go doesn't seem to have labrea scale tarpits, only certain conventional mudwallows. They absolutely love tabs and will insert them into your code for you if you fail to love them enough.\n\n## 3. Bloom\n[Bloom][6] is a new domain-specific language for distributed programming. The current alpha release is embedded in Ruby, and targeted at early adopters. Bloom leverages new research on \"CALM\" analysis to provide tools that pinpoint distributed consistency and coordination issues in your code.\n\n## 4. Python\n[Parallel Python][7] is a python module which provides mechanism for parallel execution of python code on SMP (systems with multiple processors or cores) and clusters (computers connected via network):\n\n**Features:** \n\n * Parallel execution of python code on SMP and clusters \n * Easy to understand and implement job-based parallelization technique (easy to convert serial application in parallel)\n * Automatic detection of the optimal configuration (by default the number of worker processes is set to the number of effective processors)\n * Dynamic processors allocation (number of worker processes can be changed at runtime)\n * Low overhead for subsequent jobs with the same function (transparent caching is implemented to decrease the overhead)\n * Dynamic load balancing (jobs are distributed between processors at runtime)\n * Fault-tolerance (if one of the nodes fails tasks are rescheduled on others)\n * Auto-discovery of computational resources\n * Dynamic allocation of computational resources (consequence of auto-discovery and fault-tolerance) \n * SHA based authentication for network connections\n * Cross-platform portability and interoperability (Windows, Linux, Unix, Mac OS X)\n * Cross-architecture portability and interoperability (x86, x86-64, etc.)\n * Open source\n\nOne can get a quick idea of how the code might look by [looking at the quick-start guide for clusters][8].\n\n## 5. Reia\n[Reia][9] is a scripting language for distributed system:\n\n> Reia aims to expose the powerful\n> capabilities of Erlang in a way which\n> is easier for your average programmer\n> to understand. It aims to bring the\n> beauty and simplicity of Ruby, a\n> language which is easy and fun to\n> program in, to Erlang, a language\n> which very few will think of as easy\n> or fun to use.\n\n\n\n  [1]: http://www.erlang.org/\n  [2]: http://en.wikipedia.org/wiki/Erlang_%28programming_language%29\n  [3]: http://www.erlang.org/doc/reference_manual/distributed.html\n  [4]: http://en.wikipedia.org/wiki/Functional_programming\n  [5]: http://blog.golang.org/2011/04/go-at-heroku.html\n  [6]: http://bloom-lang.net/\n  [7]: http://www.parallelpython.com/\n  [8]: http://www.parallelpython.com/content/view/15/30/#QUICKCLUSTERS\n  [9]: http://reia-lang.org/","tags":["distributed System"],"categories":["Tech"]},{"title":"Troubleshooting mixed","url":"/2018/01/01/Troubleshooting/","content":"\nRecording some troubleshooting\n\n# Docker \nSome tips or little usage of docker \n\n1. how to shrink the docker logs\n    - `truncate -s 0 /var/lib/docker/containers/*/*-json.log` \n    - Though it works, but it still looks so terrible when your produced too many\n\n2. shadowsocks docker-compose:\n\n\n```\n---\nversion: \"3\"\n\nservices:\n\n  aes-256-cfb_ss:\n    image: mritd/shadowsocks\n    ports:\n      - 2333:8989\n    restart: always\n    command: -m \"ss-server\" -s \"-s 0.0.0.0 -p 8989 -m aes-256-cfb -k mypassword--fast-open\"\n```\n\n# netdata\n\nHere is the docker-compose file:\n\n```\nversion: '3'\n\nservices:\n  netdata:\n    image: netdata/netdata\n    volumes: \n      - /proc:/host/proc:ro\n      - /sys:/host/sys:ro\n      - /var/run/docker.sock:/var/run/docker.sock:ro\n    restart: always\n    cap_add:\n      - SYS_PTRACE\n    security_opt:\n      - apparmor=unconfined\n    ports:\n      - 19999:19999\n\n```\n\n# Python snippets\n\n反转字符串：\n```\ndef main(word):\n    return ' '.join(word.split()[::-1])\nword = 'the sky is blue'\ninverse = main(word)\nprint(word)\nprint(inverse)\n```\n\n编辑距离\n```\nimport re\nfrom collections import Counter\n\nf = open(r'word_freq.txt', encoding='utf8')\nWORDS = {}\nid = 0\nfor line in f.readlines():\n    if id % 2 == 0:\n        word_freq = line.split('\\t')\n        WORDS[word_freq[0]] = int(word_freq[1])\n    id = id + 1\nletters = open(r'word.txt', encoding='utf8').read()\n\nN = sum(WORDS.values())\n\ndef P(word):\n    return WORDS[word] / N\n\ndef know(words):\n    return set(w for w in words if w in WORDS)\n\ndef edits1(word):\n    #letters = 'abcdefghijklmnopqrstuvwxyz'\n    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)] #切分\n    deletes = [L + R[1:] for L, R in splits if R] #删除\n    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1] #移位\n    replaces = [L + c + R[1:] for L, R in splits for c in letters] #代替\n    inserts = [L + c + R for L, R in splits for c in letters] #插入\n    return set(deletes + transposes + replaces + inserts)\n\ndef edits2(word):\n    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n\ndef candidates(word):\n    return (know([word]) or know(edits1(word)) or know(edits2(word)) or [word])\n\ndef correction(word):\n    return max(candidates(word), key=P)\n\nprint(correction('正分夺秒'))\n```\n\nLoad json file and write it for utf-8 format:\n\n```\nimport os\nimport json\nimport codecs\n\ndef write_json(output):\n    with codecs.open(\"config.json\", \"w\", 'utf-8') as file:\n        # using utf-8 and ensure_ascii for keep the formatting correct\n        file.write(json.dumps(output, ensure_ascii=False))\n\n\ndef load_json():\n    with open(\"template.json\") as file:\n        template = json.load(file)\n        output = template\n        write_json(output)\n```\n\n\nMerge dictionary:\n\n1. Using the method update()\nBy using the method update() in Python, one list can be merged into another. But in this, the second list is merged into the first list and no new list is created. It returns None.\nExample:\nPython code to merge dict using update() method\n\n```\ndef Merge(dict1, dict2):\n    return(dict2.update(dict1))\n\n\n# Driver code\ndict1 = {'a': 10, 'b': 8}\ndict2 = {'d': 6, 'c': 4}\n\n# This return None\nprint(Merge(dict1, dict2))\n\n# changes made in dict2\nprint(dict2)\n```\n\n2. Using ** in Python\n\nThis is generally considered a trick in Python where a single expression is used to merge two dictionaries and stored in a third dictionary. The single expression is **. This does not affect the other two dictionaries. ** implies that the argument is a dictionary. Using ** [double star] is a shortcut that allows you to pass multiple arguments to a function directly using a dictionary. For more information refer ** kwargs in Python. Using this we first pass all the elements of the first dictionary into the third one and then pass the second dictionary into the third. This will replace the duplicate keys of the first dictionary.\nExample:\nPython code to merge dict using a single\n\n```\ndef Merge(dict1, dict2):\n    res = {**dict1, **dict2}\n    return res\n\n\n# Driver code\ndict1 = {'a': 10, 'b': 8}\ndict2 = {'d': 6, 'c': 4}\ndict3 = Merge(dict1, dict2)\nprint(dict3)\n```\n### Use argparse.\n\nFor example, with test.py:\n```\nimport argparse\n\nparser=argparse.ArgumentParser(\n    description='''My Description. And what a lovely description it is. ''',\n    epilog=\"\"\"All's well that ends well.\"\"\")\nparser.add_argument('--foo', type=int, default=42, help='FOO!')\nparser.add_argument('bar', nargs='*', default=[1, 2, 3], help='BAR!')\nargs=parser.parse_args()\n```\nRunning\n\n`% test.py -h`\nyields\n```\nusage: test.py [-h] [--foo FOO] [bar [bar ...]]\n\nMy Description. And what a lovely description it is.\n\npositional arguments:\n  bar         BAR!\n\noptional arguments:\n  -h, --help  show this help message and exit\n  --foo FOO   FOO!\n\nAll's well that ends well.\n```\n\n# Linux\n\n## What is sudo\n\nTo explain this you need to know what the programs do:\n\n`su` - The command `su` is used to switch to another user (**s** witch **u** ser), but you can also switch to the root user by invoking the command with no parameter. `su` asks you for the password of the user to switch, after typing the password you switched to the user's environment.     \n\n`sudo` - `sudo` is meant to run a single command with root privileges. But unlike `su` it prompts you for the password of the current user. This user must be in the sudoers file (or a group that is in the sudoers file). By default, Ubuntu \"remembers\" your password for 15 minutes, so that you don't have to type your password every time.\n\n`bash` - A text-interface to interact with the computer. It's important to understand the difference between login, non-login, interactive and non-interactive shells:\n\n- login shell: A login shell logs you into the system as a specified user, necessary for this is a username and password. When you hit <kbd>ctrl</kbd>+<kbd>alt</kbd>+<kbd>F1</kbd> to login into a virtual terminal you get after successful login a login shell.\n- non-login shell: A shell that is executed without logging in, necessary for this is a currently logged-in user. When you open a graphic terminal in gnome it is a non-login shell.\n- interactive shell: A shell (login or non-login) where you can interactively type or interrupt commands. For example a gnome terminal.\n- non-interactive shell: A (sub)shell that is probably run from an automated process. You will see neither input nor output.\n\n\n**`sudo su`** Calls `sudo` with the command `su`. Bash is called as interactive non-login shell. So bash only executes `.bashrc`. You can see that after switching to root you are still in the same directory:\n\n    user@host:~$ sudo su\n    root@host:/home/user#\n\n**`sudo su -`** This time it is a login shell, so `/etc/profile`, `.profile` and `.bashrc` are executed and you will find yourself in root's home directory with root's environment.\n\n**`sudo -i`** It is nearly the same as `sudo su -` The -i (simulate initial login) option runs the shell specified by the password database entry of the target user as a login shell.  This means that login-specific resource files such as `.profile`, `.bashrc` or `.login` will be read and executed by the shell.\n\n**`sudo /bin/bash`** This means that you call `sudo` with the command `/bin/bash`. `/bin/bash` is started as non-login shell so all the dot-files are not executed, but bash itself reads `.bashrc` of the calling user. Your environment stays the same. Your home will not be root's home. So you are root, but in the environment of the calling user.\n\n**`sudo -s`** reads the `$SHELL` variable and executes the content. If `$SHELL` contains `/bin/bash` it invokes `sudo /bin/bash` (see above).\n\n\n**Check:**\nTo check if you are in a login shell or not (works only in bash because `shopt` is a builtin command):\n\n    shopt -q login_shell && echo 'Login shell' || echo 'No login shell'\n\n\n\n### Referencing \n- https://askubuntu.com/questions/376199/sudo-su-vs-sudo-i-vs-sudo-bin-bash-when-does-it-matter-which-is-used\n\n### To enable root login in ubuntu: \n\nset `/etc/ssh/sshd_config`:\n```\nPermitRootLogin yes\n```\n\n# Jupyter notebook\n\n[Official Guild](https://jupyter-notebook.readthedocs.io/en/stable/public_server.html)\n\n[Running as a daemon](https://stackoverflow.com/questions/14297741/how-to-start-ipython-notebook-server-at-boot-as-daemon)\n\nIf you are using Anaconda integrated jupyter-notebook, using `which jupyter-notebook` to find the place that jupyter located.\n\nThen add this to `/usr/lib/systemd/system/jupyter.service`\n\n```\n[Unit]\nDescription=Jupyter Notebook\n\n[Service]\nType=simple\nPIDFile=/run/jupyter.pid\nExecStart=/root/anaconda3/bin/jupyter-notebook --config=/root/.jupyter/jupyter_notebook_config.py --allow-root\nUser=jupyter\nGroup=jupyter\nWorkingDirectory=/root\nRestart=always\nRestartSec=10\n#KillMode=mixed\n\n[Install]\nWantedBy=multi-user.target\n```\n\nSometimes it will faild by the Permission or config issues, using the following method to solve it.\n\n- add `--allow-root` in the end of the ExecStart command\n- remove `User` or `Group` on `[Service]` Option\n- add `/bin/bash -c ` in the head of the ExecStart\n\n\n# Windows\n\nWhen I transfer my development environment from OSX to Windows, due to the stipulation, I stuffered a lot because of the notorious unfriendly dev-env of windows.\n1. Unable to create any file or folder. Even though I reset the Security on the Properties on the Disk to full control. \n    - Finally I figured out some sorfware or my slip to create redgit lose. \n    - use win + R to open regdit\n    - Go into `HKEY_CLASSES_ROOT\\Directory\\Background\\shellex\\ContextMenuHandlers\\`\n    - Later I will find there only two option in the Context Menu Handlers. That's werid.\n    - If New folder exists, edits the New option and modify the Value to `{D969A300-E7FF-11d0-A93B-00A0C90F2719}`\n    - If it not exists, create one and fill the Value as I mentioned above.\n    \n\n# Node.js\n1. Though there are planty of method to deal with the callback machinism, still too many people, especially who have been learning static type language with no rudimentary of asynchronous programming experience, cannot be capable of adapting subversion coding style.\n2. So I am about to record some trouble when I learning node.js or typescripts or other javascripts-like stuff as a `story`.\n3. When I use vscode to write node.js, more specifically, to write javascript style language, like typescript.\n    - but when I use `buffer` type on typescript, there are always pop up a red dot which notice me `[ts] cannot find name 'buffer'`. That's weird since buffer is a normal variable type in the most programming language, so is it in typescript. However, it still annoying me. \n    - Here is the conclusion: some node environmnet types are needed when use typescript, for some types are not embeded in typescript. For this limitation, we might be able to get around by stubbing those interfaces in the future. \n    - I admit it's odd to have to include node typing for a fornt-end project, but when I try install the envrionmetn typeings to see if it gets around on my issues. It workd for now. \n    - `npm install --save-deve @types/node`\n4. This day, I try to use RocketMQ to build a connection between nodejs and Java, nothing happends... \n5. When I using npm install to install grpc, processes stucked in `node-pre-gyp install --fallback-to-build --library=static_library`. Waiting over than 1 hours or more still cannot finished that course. Besides, when I checked the cpu usage of the machine, it seems that all the processers are running in low load, which means that the machine works fine. Now that the trouble must caused by the npm. It looks like shortage of some optional package which makes the process stagged. \n    - using `yum install -y node-gyp` to install the node-pre-gyp or somethings else(I am not sure what it is. But when all installation stuff done, then the npm install going smoothly without any hitch). It is normal when the npm install slow down or even unresponsive when `node-pre-grp install`, after all the build progress consume some system resources.\n6. running command with nodejs: https://stackoverflow.com/questions/20643470/execute-a-command-line-binary-with-node-js?answertab=votes#tab-top\n\n# Kubernetes\n1. when you have already installed docker-ce or other docker toolkit, you may come into installation conflict unless you choose other installation method instead of using package management tool like `yum/apt/...`. \n    - otherwise, you should uninstall the old version of docker, then run `yum install -y etcd kubernetes`\n2. when I using pod deployment to run a hello-world cluster, I found that k8s can not let the container running and unable to create replication. \n    - Use `kubectl describe pods` to get more information. Scrolling down to the last line, I saw that it's going to readhat.com and failing. Why it's using RedHat repo？ Did it should pull from the docker hub?\n\n    ```\n    Error syncing pod, skipping: failed to \"StartContainer\" for \"POD\" with ErrImagePull: \"image pull failed for registry.access.redhat.com/rhel7/pod-infrastructure:latest, this may be because there are no credentials on this request.  details: (open /etc/docker/certs.d/registry.access.redhat.com/redhat-ca.crt: no such file or directory)\"\n\n    ```\n    \n    - using `yum install -y subscription-manager rhsm` to help the k8s pull from the right place\n    - But unfortunately, I failed to install both rhsm and subscription. In fact, rhsm has been replace by RedHat's subscription-manager. It cannot works for my machine, yet subscription-manager installed successfuly.\n    - So I have to search the old version of rhsm and then use rpm installation tool to force my systemd to use rhsm.\n    - Here is the package download address: \n    - https://www.rpmfind.net/linux/centos/7.5.1804/os/x86_64/Packages/python-rhsm-certificates-1.19.10-1.el7_4.x86_64.rpm\n    - https://www.rpmfind.net/linux/centos/7.5.1804/os/x86_64/Packages/python-rhsm-1.19.10-1.el7_4.x86_64.rpm\n    - Both rhsm and certificate should installed in order, the rhsm is rely on the certificate tool. \n    - Make sure you download the two package on the same folder and use `rpm -ivh python-rhsm-*.rpm` to keep the installation in order.\n\n3. After I deal with the rhsm ERROR, kubernetes throw up another problem immediately to me, compelling me to keep dive into the OPS stuff with unprepared technology experience. \n\n    ```\n    image pull failed for registry.access.redhat.com/rhel7/pod-infrastructure:latest, this may be because there are no credentials on this request.  details: (net/http: request canceled)\n    ```\n    \n    - As plain as on the screen, kubernetes cannot interactive docker to pull a image from hub. Most of the solution that I used for settle is using the proxy(To bypass the GWF).\n    - Using `docker pull registry.access.redhat.com/rhel7/pod-infrastructure:latest`, it looks good to me.\n\n4. If you choose `hostpath` as your presistentVolume, you can only use `ReadWriteOnce` for single node, since you developments environment is single node.\n\n5. And if you create a service without defining a selector as in a yaml file, like most people do in case to select the pods, endpoints will not be create. But, fortunatly, kubernetes offers a alternative way to make sure you container, which deployed in some specific pods, to communicate with other exogenous container. \n    - declaring another kind as endpoints, as you can noticed when you use `kubectl get endpoints`, which endpoints defined as a permanent word in kubetctl, to create a type of component, with setting some propetries.\n    - when it comes to the imperative complexity, multiple ports is be requisted in many services deployment. As defined in whether kubernetes or any network policy, exposing more than one port is necessary. Kubernetes supports multiple port definitions on a Service object. Resulting from that predefined statements, when using multiple ports you must give all of your ports names, so that endpoints can be disambiguated.  \n    - Endpoint will not be alloted unless the pods be created.\n\n6. helm init failure becuase of the image pulling crash. So I need to setting a docker-proxy and daemon-reload/restart docker to pull the images again.\n7. Though helm init successfully when image has been pulled, helm still cannot connect to the kubernetes api server. It shows:\n    ```\n    Error: Get http://localhost:8080/api/v1/namespaces/kube-system/configmaps?labelSelector=OWNER%!D(MISSING)TILLER: dial tcp [::1]:8080: connect: connection refused\n    ```\n    - It seems that the tiller pod in kubernetes cannot connect to the api servier cause lack of HOST or DNS. \n    - Using `docker exec -it container_ip env` to get the environment variable to check whether the KUBERNETES_SERVICE_HOST exists. Then I got the 10.254.0.1, it looks like the default kuberentes service IP\n8. In the Great China, most of the Internet stuff about the goolge cannot be accessible. But some tech company are willing to help other programmer to get access google cloud, npm and any repo which is blocked in the outside of the GWF. It is the same as the helm. The final solution for me is installing an aliyun version of helm to get to start the tiller. Also, don't forget to get the right version matching between client and server.\n    ```\n    helm init --upgrade -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.9.1 --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts\nkubectl create serviceaccount --namespace kube-system tiller\nkubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller\nkubectl patch deploy --namespace kube-system tiller-deploy -p '{\"spec\":{\"template\":{\"spec\":{\"serviceAccount\":\"tiller\"}}}}'\n    ```\n\n9. Noticed that when I using 2.10 version of helm, there is an gorouting error happend by typing `helm ls`\n\n    ```\n    panic: runtime error: invalid memory address or nil pointer dereference [signal SIGSEGV: segmentation violation code=0x1 addr=0x30 pc=0x1276506] tiller\n    ```\n    - That is a dev bug undering the development of the helm.\n    - downupgrde you helm to 2.9.1 will be fine.\n\n# Git\n1. `Encountered end of file`\n    - Add $GIT_CURL_VERBOSE=1 can solved that problem \n2. Still be annoied by the useless private gitlab server, problem comes one by one without any obviating indication. \n    - `Peer's Certificate issuer is not recognized.`\n    - two solutions, the first one is setting GIS_SSL_NO_VERIFY environment parametes to true\n    - the another one is using `git config --global http.sslVerify false` to solving the config missing\n3. git can not clone a repo from a unsafe link \n    ```\n    fatal: unable to access 'https://***.git/': SSL certificate problem: self signed certificate\n    ```\n\n    - solution: add a environment in the front of you command:\n    - GIT_SSL_NO_VERIFY=true git clone ****\n4. what if you used a Github account on your mac before but now your changed your account?\n\nTry remove your account:\n\n```\n$ git credential-osxkeychain erase\nhost=github.com\nprotocol=https\n[Press Return]\n```\nand using `git push`  or whatever the command to handle git and login your git account again.\n\n# Hyperledger Fabric\n1. `Error creating GRPC server: listen tcp: lookup [your-peer-name] on 127.0.0.11:53: no such host`\n    - Small fix difícil de conseguir para Hyperledger Fabric 1.1.0 corriendo en Docker Swarm. Multiples organizations corriendo en un mismo host:\n    ```\n    createChaincodeServer -> ERRO 02a Error creating GRPC server: listen tcp: lookup [your-peer-name] on 127.0.0.11:53: no such host\n\n    Resulta que para la versión 1.1.0 de HLF se requiere incluir esta propiedad en el environment section del docker-compose.yaml de los peers:\n\n    CORE_PEER_CHAINCODELISTENADDRESS=0.0.0.0:7052\n    ```\n\n2. `Error: Error: Calling enrollment endpoint failed with error [Error: read ECONNRESET]`\n    - When I try to monitor a event hub for chaincode calling in node SDK, some error throw up after I use my own script to create channel, join, and other stuff. \n    - The weird things is that, checking the ca container looks good to me. Then I recalled that if it is my fault that not to add remove temp file cache in scripts. So I rechecked it again, however, I cleared the cache. \n    - How can this happend without any change I did to the server. So I debug my script by line: after I started up the network, I did other channel stuff in few seconds. It surprised me that the log did not throw the error mentioned above. \n    - It turns out that I can create the channel immediately when I started up the docker-compose.\n    - Add sleep 5 to the scripts will be fine.\n\n3. `Promise is rejected: Error: 2 UNKNOWN: access denied: channel [mychannel] creator org [Org1MSP]`\n    - orgpeercan not be found in channel\n4. `error executing chaincode: failed to execute transaction: timeout expired while executing transaction`\nProbably because of the chaincode is not installed perfectly","tags":["troubleshooting, docker, Hyperledger, linux"],"categories":["Tech"]},{"title":"Coursera Certificate","url":"/2016/12/31/Coursera-Certificate/","content":"\nThis page is used for storing coursera certificate:\n\n- [Business Metrics for Data-Driven Companies](https://www.coursera.org/account/accomplishments/certificate/3FA8KTMMZAE8)\n- [Business - Customer Analytics](https://www.coursera.org/account/accomplishments/certificate/2ARVFK73G7TD)\n- [Data Science in Python](https://www.coursera.org/account/accomplishments/certificate/9K397R9VGJP9)\n- [The Data Scientist’s Toolbox](https://www.coursera.org/account/accomplishments/certificate/2ZYVT2V9EJVT)\n- [Databases with Python](https://www.coursera.org/account/accomplishments/certificate/AXSLNFRFVEJW)\n- [Web Python](https://www.coursera.org/account/accomplishments/certificate/JTCJXZEXS4BK)\n- [Python Data Structure](https://www.coursera.org/account/accomplishments/certificate/VP3EJGFDKWU4)\n- [Getting and Cleaning Data](https://www.coursera.org/account/accomplishments/certificate/H6S7KS99RTJP)\n- [R Programming](https://www.coursera.org/account/accomplishments/certificate/24PB885RTJXG)"},{"title":"The odyssey of Self-Discipline","url":"/2016/12/30/The-odyssey-of-Self-Discipline/","content":"## Why I am so hard to stay disciplined?\n\nI'd known I am one of the ADHDer(not so heavy, but just always losing my focus). The worst side of the influence is not the symptom (never ever) but the untangible mind which unconciously affect my behavior. I learned a word -- preconception and suddenly be enlightened.\n\nI should be more self discipline and be more confident about my mind. \n\nBut why it is still so hard to stay disciplined?\n\n> Because we allow our mind to interface the path of discipline. Why do we fail to get in the early morning? Because the mind says, \"You are so tired and sleep deprived. Just sleep for 5 minute more and it will be fine\". Then % becomes 10, and it is all the way downhill after that.\n\nWe allow the fallacy that we will get up early or eat healthy or study well or do the right thing when we **get used to it and it becomes a little easier**. \n\nBut it will NEVER get easier. It will never be easy to get up at 5 am, every single day of the year. Not now, not after 10 years. \n\n## What should I do?\nThe answer maybe trivially simple, but also very hard. I must stop listening to my mind. Just stop. No concession to what my mind says.\n\nWhen I have to do something hard, I have to become a robot. It does not matter how I feel. Who cares? It does not matter how this thing will get well. Becuase it is a training, not just a task. I must get used to it and improve the way I used for handling stuff. At first, I just execute like the plan like a machine. \n\nThen I decide to go for a run, get up and run. I feel groggy and my legs hurt. The pain I feel is exactly the pain everybody else feels. The pain of discipline is the price of happiness. \n\n\n### People who win are those who keep going, regardless of how much it hurts.\n\n","tags":["methdology, efficiency"],"categories":["Lifestyle"]}]